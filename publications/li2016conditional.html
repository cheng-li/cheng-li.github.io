<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="google-site-verification"
      content="XmgClnbEKziAyxnX-L7KcLkvKpPrGYHE2e0AWyoi2oE">
    <!-- Metadata -->
    <!-- Google Scholar Meta Data -->
    <meta name="citation_title" content="Conditional Bernoulli Mixtures
      for Multi-label Classification">
    <meta name="citation_author" content="Li, Cheng">
    <meta name="citation_author" content="Wang, Bingyu">
    <meta name="citation_author" content="Pavlu, Virgil">
    <meta name="citation_author" content="Aslam, Javed A.">
    <meta name="citation_publication_date" content="2016">
    <meta name="citation_conference_title" content="Proceedings of The
      33rd International Conference on Machine Learning">
    <meta name="citation_pdf_url"
      content="http://www.chengli.io/publications/li2016conditional.pdf">
    <title>Conditional Bernoulli Mixtures for Multi-label Classification</title>
  </head>
  <body>
    <h2>Abstract</h2>
    Multi-label classification is an important machine learning task
    wherein one assigns a subset of candidate labels to an object. In
    this paper, we propose a new multi-label classification method based
    on
    Conditional Bernoulli Mixtures. Our proposed method has
    several attractive properties: it captures label dependencies; it is
    efficient both in
    training and in prediction; it reduces the multi-label problem to
    several standard binary and multi-class problems; it subsumes the
    classic independent binary prediction and power-set subset
    prediction
    methods as special cases; and it exhibits accuracy and/or
    computational complexity advantages over sophisticated approaches
    such
    as classifier chains, conditional dependency networks, and
    conditional
    random fields.
    <br>
    <br>
    We demonstrate two implementations using logistic regression and
    gradient boosted trees, together with training procedures based on
    Expectation Maximization. We further derive an efficient prediction
    procedure based on dynamic programming, thus avoiding the cost of
    examining an exponential number of potential label
    subsets. Experimental results show the effectiveness of the proposed
    method against competitive alternatives on benchmark multi-label
    datasets.
    <h2>Related Material</h2>
    <div id="extras">
      <ul>
        <li><a href="li2016conditional.pdf">PDF</a></li>
        <li><a
href="http://www.chengli.io/publications/li2016conditional_supplementary.pdf">Supplementary
            Material<br>
          </a></li>
        <li><a href="li2016conditional_poster.pdf">Poster</a></li>
        <li><a href="li2016conditional.bib">BibTex</a><br>
        </li>
      </ul>
    </div>
    <!-- extras -->
  </body>
</html>
